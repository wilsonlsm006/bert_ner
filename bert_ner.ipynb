{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#! -*- coding: utf-8 -*-\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras_bert import load_trained_model_from_checkpoint, Tokenizer\n",
    "import codecs\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "#BERT的相关参数\n",
    "mode = 0\n",
    "maxlen = 300\n",
    "learning_rate = 5e-5\n",
    "min_learning_rate = 1e-6\n",
    "\n",
    "config_path = '/home/notebook/data/group/otext/bert_model/bert_config.json'\n",
    "checkpoint_path = '/home/notebook/data/group/otext/bert_model/bert_model.ckpt'\n",
    "dict_path = '/home/notebook/data/group/otext/bert_model/vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = {}\n",
    "\n",
    "# 加载词表\n",
    "with codecs.open(dict_path, 'r', 'utf8') as reader:\n",
    "    for line in reader:\n",
    "        token = line.strip()\n",
    "        token_dict[token] = len(token_dict)\n",
    "\n",
    "\n",
    "class OurTokenizer(Tokenizer):\n",
    "    # 定制化分词器，这里不论中文还是英文都根据单个字符进行切分\n",
    "    def _tokenize(self, text):\n",
    "        R = []\n",
    "        for c in text:\n",
    "            if c in self._token_dict:\n",
    "                R.append(c)\n",
    "            elif self._is_space(c):\n",
    "                R.append('[unused1]') # space类用未经训练的[unused1]表示\n",
    "            else:\n",
    "                R.append('[UNK]') # 剩余的字符是[UNK]\n",
    "        return R\n",
    "\n",
    "# 构造分词器实例\n",
    "tokenizer = OurTokenizer(token_dict)\n",
    "\n",
    "def seq_padding(X, padding=0):\n",
    "    # 填充补0\n",
    "    L = [len(x) for x in X]\n",
    "    ML = max(L)\n",
    "    return np.array([\n",
    "        np.concatenate([x, [padding] * (ML - len(x))]) if len(x) < ML else x for x in X\n",
    "    ])    \n",
    "\n",
    "def list_find(list1, list2):\n",
    "    # 在list1中查找子串list2，如果找到返回初始的下标，否则返回-1\n",
    "    n_list2 = len(list2)\n",
    "    for i in range(len(list1)):\n",
    "        if list1[i: i+n_list2] == list2:\n",
    "            return i\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>unknownEntities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83dcefb7</td>\n",
       "      <td>揭秘趣步骗局，趣步是什么，趣步是怎么赚钱的？趣步公司可靠吗？趣步合法吗？相信是众多小伙伴最关...</td>\n",
       "      <td>揭秘趣步骗局，趣步是什么，趣步是怎么赚钱的？趣步公司可靠吗？趣步合法吗？相信是众多小伙伴最关...</td>\n",
       "      <td>趣步</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ad5be0d</td>\n",
       "      <td>企业纳税贷额度，全国小微企业都可做！</td>\n",
       "      <td>{IMG:1}{IMG:2}公司张总说：“没想到缴税还能办贷款，本来我们还在为准备纳税证明、...</td>\n",
       "      <td>西部助贷</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6dd28e9b</td>\n",
       "      <td>一线|新控股股东入主后联讯证券拟改名为知识城证券</td>\n",
       "      <td>腾讯新闻《》作者刘鹏成功接下47.24%股权入主具有30年历史的联讯证券后，广州开发区金融控...</td>\n",
       "      <td>广州开发区金融控股集团有限公司</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              title  \\\n",
       "0  83dcefb7  揭秘趣步骗局，趣步是什么，趣步是怎么赚钱的？趣步公司可靠吗？趣步合法吗？相信是众多小伙伴最关...   \n",
       "1  1ad5be0d                                 企业纳税贷额度，全国小微企业都可做！   \n",
       "2  6dd28e9b                           一线|新控股股东入主后联讯证券拟改名为知识城证券   \n",
       "\n",
       "                                                text  unknownEntities  \n",
       "0  揭秘趣步骗局，趣步是什么，趣步是怎么赚钱的？趣步公司可靠吗？趣步合法吗？相信是众多小伙伴最关...               趣步  \n",
       "1  {IMG:1}{IMG:2}公司张总说：“没想到缴税还能办贷款，本来我们还在为准备纳税证明、...             西部助贷  \n",
       "2  腾讯新闻《》作者刘鹏成功接下47.24%股权入主具有30年历史的联讯证券后，广州开发区金融控...  广州开发区金融控股集团有限公司  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取训练集\n",
    "#训练集字段介绍\n",
    "#id代表唯一数据标识\n",
    "#title和text是用于识别的文本，可能为空\n",
    "#unknownEntities代表实体，可能有多个，通过英文\";\"分隔\n",
    "train_data = pd.read_csv('./Train_Data.csv').fillna('>>>>>')\n",
    "train_data = train_data[~train_data['unknownEntities'].isnull()].reset_index(drop = True)\n",
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4456 557\n"
     ]
    }
   ],
   "source": [
    "# 将title和text合并成content字段，将模型转化成单输入问题\n",
    "# 如果title和text字段相等那么就合并，否则返回其中一个就行了\n",
    "train_data['content'] = train_data.apply(lambda x: x['title'] if x['title']==x['text'] else x['title']+x['text'], axis = 1)\n",
    "\n",
    "# 对于unknownEntities字段中存在多个实体的只使用第一个实体\n",
    "train_data['unknownEntity'] = train_data['unknownEntities'].apply(lambda x:x.split(';')[0])\n",
    "\n",
    "# 获取所有的实体类别\n",
    "# 这里先将unknownEntities进行拼接，然后根据\";\"切分\n",
    "entity_str = ''\n",
    "for i in train_data['unknownEntities'].unique():\n",
    "    entity_str = i + ';' + entity_str  \n",
    "    \n",
    "entity_classes_full = set(entity_str[:-1].split(\";\"))\n",
    "# 3183\n",
    "len(entity_classes_full)\n",
    "\n",
    "# 训练集变成了两个字段：\n",
    "# 需要识别的文本content，这是原始数据集中title和text合并之后的数据\n",
    "# 未知实体列表unknownEntities，类似于label，只会有一个实体\n",
    "train_data_list = []\n",
    "for content,entity in zip(train_data['content'], train_data['unknownEntity']):\n",
    "    train_data_list.append((content, entity))\n",
    "    \n",
    "# 根据9:1划分训练集和验证集    \n",
    "random_order = np.arange(len(train_data_list))\n",
    "train_list = [train_data_list[j] for i, j in enumerate(random_order) if i % 9 != mode]\n",
    "dev_list = [train_data_list[j] for i, j in enumerate(random_order) if i % 9 == mode]\n",
    "print(len(train_list), len(dev_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备测试集数据\n",
    "test_data = pd.read_csv('./Test_Data.csv').fillna('>>>>>')\n",
    "test_data['content'] = test_data.apply(lambda x: x['title'] if x['title']==x['text'] else x['title']+x['text'], axis = 1)\n",
    "\n",
    "\n",
    "# 测试集变成了两个字段：\n",
    "# 控制数据唯一性的id\n",
    "# 需要识别的文本content，这是原始数据集中title和text合并之后的数据\n",
    "test_data_list = []\n",
    "for id,content in zip(test_data['id'], test_data['content']):\n",
    "    test_data_list.append((id, content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ',\n",
       " '&',\n",
       " '(',\n",
       " ')',\n",
       " '+',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '>',\n",
       " '?',\n",
       " '·',\n",
       " '“',\n",
       " '”',\n",
       " '（',\n",
       " '）',\n",
       " '：'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 找到训练集中content字段中文、英文和数字以外的特殊字符\n",
    "additional_chars = set()\n",
    "for data in train_data_list:\n",
    "    additional_chars.update(re.findall(u'[^\\u4e00-\\u9fa5a-zA-Z0-9\\*]', data[1]))\n",
    "additional_chars    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_data_generator:\n",
    "    \"\"\"\n",
    "    训练集数据生成器\n",
    "    \"\"\"\n",
    "    def __init__(self, train_list, batch_size=32):\n",
    "        self.train_list = train_list\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = len(self.train_list) // self.batch_size\n",
    "        if len(self.train_list) % self.batch_size != 0:\n",
    "            self.steps += 1\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            # 返回训练数据集中的索引列表\n",
    "            idxs = np.arange(len(self.train_list))\n",
    "            np.random.shuffle(idxs)\n",
    "            X1, X2, S1, S2 = [], [], [], []\n",
    "            for i in idxs:\n",
    "                train = self.train_list[i]\n",
    "                # 这里对于超长的文本只会取前510个字符\n",
    "                # 业界还有一种取头和取尾的方法，思想主要是一篇文章中头部和尾部的内容重要性更高\n",
    "                # head + tail ： 选择前128个 token 和最后382个 token\n",
    "                # text= train[0][:128] + train[0][382:]\n",
    "                text= train[0][:maxlen]\n",
    "                \n",
    "                tokens = tokenizer.tokenize(text)\n",
    "                # entity代表实体\n",
    "                entity = train[1]\n",
    "                \n",
    "                # 获取实体的字符,因为首尾是cls和sep，所以取[1:-1]\n",
    "                e_tokens = tokenizer.tokenize(entity)[1:-1]\n",
    "                entity_left_np, entity_right_np = np.zeros(len(tokens)), np.zeros(len(tokens))\n",
    "                \n",
    "                # 返回e_tokens实体在tokenszi字符串中的起始位置\n",
    "                start = list_find(tokens, e_tokens)\n",
    "                if start != -1:\n",
    "                    end = start + len(e_tokens) - 1\n",
    "                    entity_left_np[start] = 1\n",
    "                    entity_right_np[end] = 1\n",
    "                    \n",
    "                    # x1是词编码，x2是句子对关系编码\n",
    "                    word_embedding, seg_embedding = tokenizer.encode(first=text)\n",
    "                    X1.append(word_embedding)\n",
    "                    X2.append(seg_embedding)\n",
    "                    \n",
    "                    # 对于文本分类来说，S1和S2代表标签\n",
    "                    # 这里命名体识别任务S1、S2代表文本中的实体左右边界\n",
    "                    # 比如 tokens=['[CLS]', '傻', '大', '姐', '借', '口', '给', '二', '妹', '送', '钱', 'love', '[SEP]'] \n",
    "                    # e_tokens = ['二', '妹']\n",
    "                    # word_embedding 是text编码得到的词编码   [101, 1004, 1920, 1995, 955, 1366, 5314, 753, 1987, 6843, 7178, 8451, 102]\n",
    "                    # seg_embedding 是text编码得到的句子对编码   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "                    # s1是数组中实体开始位置为1其他均为0 [array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])]\n",
    "                    # s2是数组中实体结束为只为1其他均为0 [array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])]                    \n",
    "                    S1.append(entity_left_np)\n",
    "                    S2.append(entity_right_np)\n",
    "                    if len(X1) == self.batch_size or i == idxs[-1]:\n",
    "                        X1 = seq_padding(X1)\n",
    "                        X2 = seq_padding(X2)\n",
    "                        S1 = seq_padding(S1)\n",
    "                        S2 = seq_padding(S2)\n",
    "                        yield [X1, X2, S1, S2], None\n",
    "                        X1, X2, S1, S2 = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "word_in (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "seg_in (InputLayer)             (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_8 (Model)                 (None, None, 768)    101677056   word_in[0][0]                    \n",
      "                                                                 seg_in[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "ps1 (Dense)                     (None, None, 1)      768         model_8[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "x_mask (Lambda)                 (None, None, 1)      0           word_in[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "ps2 (Dense)                     (None, None, 1)      768         model_8[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "ps11 (Lambda)                   (None, None)         0           ps1[0][0]                        \n",
      "                                                                 x_mask[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "ps22 (Lambda)                   (None, None)         0           ps2[0][0]                        \n",
      "                                                                 x_mask[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 101,678,592\n",
      "Trainable params: 101,678,592\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/engine/training_utils.py:819: UserWarning: Output ps11 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to ps11.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n",
      "/opt/conda/lib/python3.7/site-packages/keras/engine/training_utils.py:819: UserWarning: Output ps22 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to ps22.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    }
   ],
   "source": [
    "# 构建训练模型\n",
    "# 整个模型是单输入和单输出的问题\n",
    "# 模型输入是一条query文本，这里会先将文本转换成三层embedding，token embedding、seg embedding和position embedding\n",
    "# 因为句子关系可以直接获取，所以只返回token embedding、seg embedding两个输入，作为网络的输入\n",
    "# 模型输出是一个实体，这个实体是query中的一个子片段\n",
    "#根据这个输出特性，输出应该用指针结构，通过两个Softmax分别预测首尾，然后得到一个实体\n",
    "# 所以这里返回实体的左边界和右边界作为网络的输出\n",
    "\n",
    "# 导入预训练模型\n",
    "bert_model = load_trained_model_from_checkpoint(config_path, checkpoint_path, seq_len=None)\n",
    "\n",
    "# 是否进行微调\n",
    "for layer in bert_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# 词编码输入\n",
    "word_in = Input(shape=(None,), name='word_in') \n",
    "# 句子对编码输入\n",
    "seg_in = Input(shape=(None,), name='seg_in')\n",
    "# 实体左边界数组，只有实体开始位置为1，其他均为0\n",
    "entiry_left_in = Input(shape=(None,), name='entiry_left_in')\n",
    "# 实体右边界数组，只有实体结束位置为1，其他均为0\n",
    "entiry_right_in = Input(shape=(None,), name='entiry_right_in')\n",
    "\n",
    "x1, x2, s1, s2 = word_in, seg_in, entiry_left_in, entiry_right_in\n",
    "\n",
    "bert_in = bert_model([word_in, seg_in])\n",
    "ps1 = Dense(1, use_bias=False, name='ps1')(bert_in)\n",
    "# 遮掩掉不应该读取到的信息，或者无用的信息，以0作为mask的标记\n",
    "x_mask = Lambda(lambda x: K.cast(K.greater(K.expand_dims(x, 2), 0), 'float32'), name='x_mask')(word_in)\n",
    "ps2 = Dense(1, use_bias=False, name='ps2')(bert_in)\n",
    "ps11 = Lambda(lambda x: x[0][..., 0] - (1 - x[1][..., 0]) * 1e10, name='ps11')([ps1, x_mask])\n",
    "ps22 = Lambda(lambda x: x[0][..., 0] - (1 - x[1][..., 0]) * 1e10, name='ps22')([ps2, x_mask])\n",
    "\n",
    "train_model = Model([word_in, seg_in, entiry_left_in, entiry_right_in], [ps11, ps22])\n",
    "\n",
    "# 构建模型\n",
    "build_model = Model([word_in, seg_in], [ps11, ps22])\n",
    "\n",
    "loss1 = K.mean(K.categorical_crossentropy(entiry_left_in, ps11, from_logits=True))\n",
    "ps22 -= (1 - K.cumsum(s1, 1)) * 1e10\n",
    "loss2 = K.mean(K.categorical_crossentropy(entiry_right_in, ps22, from_logits=True))\n",
    "loss = loss1 + loss2\n",
    "\n",
    "train_model.add_loss(loss)\n",
    "train_model.compile(optimizer=Adam(learning_rate))\n",
    "train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 经过一个softmax操作\n",
    "def softmax(x):\n",
    "    x = x - np.max(x)\n",
    "    x = np.exp(x)\n",
    "    return x / np.sum(x)\n",
    "softmax([1, 9, 5, 3])\n",
    "\n",
    "# 抽取实体\n",
    "# 输入用户搜索query\n",
    "# 输出实体\n",
    "def extract_entity(text_in):\n",
    "    text_in = text_in[:maxlen]\n",
    "    _tokens = tokenizer.tokenize(text_in)\n",
    "    _x1, _x2 = tokenizer.encode(first=text_in)\n",
    "    _x1, _x2 = np.array([_x1]), np.array([_x2])\n",
    "    _ps1, _ps2  = build_model.predict([_x1, _x2])\n",
    "    _ps1, _ps2 = softmax(_ps1[0]), softmax(_ps2[0])\n",
    "    for i, _t in enumerate(_tokens):\n",
    "        if len(_t) == 1 and re.findall(u'[^\\u4e00-\\u9fa5a-zA-Z0-9\\*]', _t) and _t not in additional_chars:\n",
    "            _ps1[i] -= 10\n",
    "    start = _ps1.argmax()\n",
    "    for end in range(start, len(_tokens)):\n",
    "        _t = _tokens[end]\n",
    "        if len(_t) == 1 and re.findall(u'[^\\u4e00-\\u9fa5a-zA-Z0-9\\*]', _t) and _t not in additional_chars:\n",
    "            break\n",
    "    end = _ps2[start:end+1].argmax() + start\n",
    "    a = text_in[start-1: end]\n",
    "    return a\n",
    "\n",
    "class Evaluate(Callback):\n",
    "    \"\"\"构建自定义评估期\"\"\"\n",
    "    def __init__(self):\n",
    "        self.ACC = []\n",
    "        self.best = 0.\n",
    "        self.passed = 0\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        \"\"\"第一个epoch用来warmup，第二个epoch把学习率降到最低\n",
    "        \"\"\"\n",
    "        if self.passed < self.params['steps']:\n",
    "            lr = (self.passed + 1.) / self.params['steps'] * learning_rate\n",
    "            K.set_value(self.model.optimizer.lr, lr)\n",
    "            self.passed += 1\n",
    "        elif self.params['steps'] <= self.passed < self.params['steps'] * 2:\n",
    "            lr = (2 - (self.passed + 1.) / self.params['steps']) * (learning_rate - min_learning_rate)\n",
    "            lr += min_learning_rate\n",
    "            K.set_value(self.model.optimizer.lr, lr)\n",
    "            self.passed += 1\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        acc = self.evaluate()\n",
    "        self.ACC.append(acc)\n",
    "        if acc > self.best:\n",
    "            self.best = acc\n",
    "            train_model.save_weights('best_model.weights')\n",
    "        print('acc: %.4f, best acc: %.4f\\n' % (acc, self.best))\n",
    "    def evaluate(self):\n",
    "        A = 1e-10\n",
    "        F = open('dev_pred.json', 'w')\n",
    "        for d in tqdm(iter(dev_list)):\n",
    "            R = extract_entity(d[0])\n",
    "            if R == d[1]:\n",
    "                A += 1\n",
    "            s = ', '.join(d + (R,))\n",
    "            F.write(s + '\\n')\n",
    "        F.close()\n",
    "        return A / len(dev_list)\n",
    "\n",
    "evaluator = Evaluate()\n",
    "train_D = train_data_generator(train_list)\n",
    "train_model.fit_generator(train_D.__iter__(),\n",
    "    steps_per_epoch=len(train_D),\n",
    "    epochs=2,\n",
    "    callbacks=[evaluator]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'大乐透'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_entity(\"今天大乐透开奖了么？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'奥迪A6'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_entity(\"奥迪A6多少钱\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3 5 大乐透\n",
      "1 4 5 乐透\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'乐透;大乐透'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 抽取实体测试\n",
    "# 输入文本\n",
    "# 返回实体列表，这里最多返回num个实体\n",
    "def extract_entity_test(model, text_in, num):\n",
    "    text_in = text_in[:maxlen]\n",
    "    _tokens = tokenizer.tokenize(text_in)\n",
    "    _x1, _x2 = tokenizer.encode(first=text_in)\n",
    "    _x1, _x2 = np.array([_x1]), np.array([_x2])\n",
    "    _ps1, _ps2  = model.predict([_x1, _x2])\n",
    "    _ps1, _ps2 = softmax(_ps1[0]), softmax(_ps2[0])\n",
    "    \n",
    "    # 特殊字符转换为负值\n",
    "    for i, _t in enumerate(_tokens):\n",
    "        if len(_t) == 1 and re.findall(u'[^\\u4e00-\\u9fa5a-zA-Z0-9\\*]', _t) and _t not in additional_chars:\n",
    "            _ps1[i] -= 10\n",
    "            \n",
    "    tg_list = list()\n",
    "    \n",
    "    for i in range(num):\n",
    "        #[0.99977237, 0.00011352481, 4.0782343e-05, 2.4224111e-05, 1.7350189e-05, 1.0297682e-05, 8.015117e-06, 6.223183e-06\n",
    "        #, 3.117688e-06, 1.7270181e-06, 1.125549e-06, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0]\n",
    "        #_ps1中的值代表为实体的概率得分，越大越可能是实体的左边界\n",
    "        #将_ps1按概率值值降序排序\n",
    "        #num代表选择topN个实体\n",
    "        start = np.argwhere((_ps1==sorted(_ps1,reverse=True)[i]))[0][0]\n",
    "        \n",
    "        # 设置中断的条件，当字符的长度为1并且为特殊字符并且不属于正常字符\n",
    "        for end in range(start, len(_tokens)):\n",
    "            _t = _tokens[end]\n",
    "            if len(_t) == 1 and re.findall(u'[^\\u4e00-\\u9fa5a-zA-Z0-9\\*]', _t) and _t not in additional_chars:\n",
    "                break\n",
    "        # _ps2中的值代表为实体的概率得分\n",
    "        # argmax()是返回_ps2最大值的索引        \n",
    "        end = _ps2[start:end+1].argmax() + start\n",
    "        a = text_in[start-1: end]\n",
    "        tg_list.append(a)\n",
    "        tg_list = list(set(tg_list))\n",
    "        print(i, start, end,a )\n",
    "    return ';'.join(tg_list)\n",
    "\n",
    "# 导入模型权重\n",
    "build_model.load_weights('best_model.weights')\n",
    "\n",
    "# 预测单个文本的实体\n",
    "extract_entity_test(build_model, '今天大乐透开奖了吗？', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
